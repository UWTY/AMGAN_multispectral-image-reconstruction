{"cells":[{"cell_type":"markdown","metadata":{"id":"oGNDNRcZFO2p"},"source":["\n","# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OX4RmK5t-YPs"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Updated Training Script for Multi-Modal Satellite Image Fusion\n","Supports L30, S1, Planet inputs with Masked operations\n","\"\"\"\n","!pip install kornia\n","from google.colab import drive\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')\n","\n","import os\n","import sys\n","import argparse\n","import time\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import kornia\n","from kornia.morphology import erosion\n","import torch.nn.functional as F\n","import platform\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","from tqdm import tqdm\n","from scipy.ndimage import binary_erosion\n","from skimage.metrics import structural_similarity as ssim\n","\n","import importlib\n","# GPU optimization\n","if torch.cuda.is_available():\n","    torch.backends.cudnn.benchmark = True\n","    device_props = torch.cuda.get_device_properties(0)\n","\n","# Replace with the actual path to the directory containing model2.py\n","model_dir_in_drive = '/content/drive/MyDrive/ColabModel/'\n","loss_dir_in_drive = '/content/drive/MyDrive/ColabLoss/'\n","\n","# Add the directory to the Python path\n","if model_dir_in_drive not in sys.path:\n","    sys.path.append(model_dir_in_drive)\n","    print(f\"Added {model_dir_in_drive} to sys.path\")\n","else:\n","    print(f\"{model_dir_in_drive} already in sys.path\")\n","sys.path.append(loss_dir_in_drive)\n","\n","# from MultimodalUnetnew import Unet\n","from MultimodalUnetnewadap import Unet\n","\n","from mse_loss import MSELoss\n","\n","torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda:0\")\n","print(f\"Using device: {DEVICE}\")"]},{"cell_type":"markdown","metadata":{"id":"5ik3U_l1FH5q"},"source":["# Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMX5CiuH_SBZ"},"outputs":[],"source":["class OptimizedPatchDataset(Dataset):\n","    \"\"\"Dataset class for loading .pt files with caching,\n","    automatically removing samples whose L30, S1, Planet mask all equal zero.\n","    \"\"\"\n","    def __init__(self, data_dir, cache_size=50, map_location=\"cpu\"):\n","        self.data_dir = Path(data_dir)\n","        all_files = sorted(self.data_dir.glob(\"sample_*.pt\"))\n","        if not all_files:\n","            raise RuntimeError(f\"[OptimizedPatchDataset] No sample_*.pt files found in {self.data_dir}\")\n","\n","        # Filter: only keep samples with at least one non-zero mask\n","        valid_files = []\n","        for p in all_files:\n","            sample = torch.load(p, map_location=map_location)\n","            m0 = sample.get('mask_l30')\n","            m1 = sample.get('mask_s1')\n","            m2 = sample.get('mask_planet')\n","            # Keep if any mask is not all zeros\n","            if not ((m0.sum()==0) and (m1.sum()==0) and (m2.sum()==0)):\n","                valid_files.append(p)\n","        if not valid_files:\n","            raise RuntimeError(\"[OptimizedPatchDataset] All samples have zero masks!\")\n","\n","        self.files = valid_files\n","        self.cache = {}\n","        self.cache_size = cache_size\n","        self.map_location = map_location\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self, idx):\n","        if idx in self.cache:\n","            return self.cache[idx]\n","        sample = torch.load(self.files[idx], map_location=self.map_location)\n","        # Type adjustment & contiguous\n","        for key, value in sample.items():\n","            if isinstance(value, torch.Tensor):\n","                if value.dtype == torch.float64:\n","                    value = value.float()\n","                sample[key] = value.contiguous()\n","        # Cache\n","        if len(self.cache) < self.cache_size:\n","            self.cache[idx] = sample\n","        return sample\n","\n","def psnr_masked(img_ref, img_test, valid_mask, data_range=1.0):\n","    \"\"\"\n","    Calculate PSNR only on pixels where valid_mask is True.\n","    img_ref:    numpy array, shape = (C, H, W)\n","    img_test:   numpy array, shape = (C, H, W)\n","    valid_mask: numpy array, shape = (H, W), bool or {0,1}\n","    data_range: float, image maximum value range (set to 1.0 if normalized to [0,1])\n","\n","    Returns: a scalar PSNR (dB). If valid_mask is all zeros, returns np.nan.\n","    \"\"\"\n","    if img_ref.shape != img_test.shape:\n","        raise ValueError(\"[psnr_masked] Input shapes are inconsistent.\")\n","    if valid_mask.shape != img_ref.shape[1:]:\n","        raise ValueError(f\"[psnr_masked] mask shape {valid_mask.shape} does not match image shape {img_ref.shape}.\")\n","\n","    I = img_ref.astype(np.float64)\n","    K = img_test.astype(np.float64)\n","\n","    # Count valid pixels\n","    num_valid_pixels = np.sum(valid_mask)\n","    if num_valid_pixels == 0:\n","        return np.nan\n","\n","    # Calculate squared error\n","    squared_error = (I - K) ** 2  # shape = (C, H, W)\n","\n","    # Expand mask to channel dimension for pixel-channel masking\n","    mask_chw = np.expand_dims(valid_mask, axis=0).repeat(I.shape[0], axis=0)\n","    masked_squared = squared_error[mask_chw > 0]\n","\n","    # MSE\n","    mse_masked = np.sum(masked_squared) / (num_valid_pixels * I.shape[0])\n","    if mse_masked <= 0:\n","        return float('inf')\n","    psnr_val = 10.0 * np.log10((data_range ** 2) / mse_masked)\n","    return float(psnr_val)\n","\n","\n","def ssim_eroded_mask(img_ref, img_test, valid_mask, max_val=1.0, **ssim_kwargs):\n","    \"\"\"\n","    Calculate SSIM with mask and erosion processing.\n","    img_ref:    numpy array, shape = (C, H, W)\n","    img_test:   numpy array, shape = (C, H, W)\n","    valid_mask: numpy array, shape = (H, W), bool or {0,1}\n","    max_val:    image maximum value range (set to 1.0 if normalized to [0,1])\n","    Returns: a scalar SSIM (averaged if multi-channel).\n","    \"\"\"\n","    # Convert to H, W, C format for skimage.metrics.ssim\n","    if img_ref.shape != img_test.shape:\n","        raise ValueError(\"[ssim_eroded_mask] Input shapes are inconsistent.\")\n","\n","    C, H, W = img_ref.shape\n","    # Convert valid_mask to bool\n","    orig_mask = valid_mask > 0\n","\n","    # Erosion: remove pixels near edges that are insufficient for window size\n","    win_size = ssim_kwargs.pop('win_size', min(3, H, W))\n","    if win_size % 2 == 0:\n","        win_size -= 1\n","    if win_size < 3:\n","        return np.nan\n","    struct_el = np.ones((win_size, win_size), dtype=bool)\n","    core_mask = binary_erosion(orig_mask, structure=struct_el, border_value=0)\n","    if np.count_nonzero(core_mask) == 0:\n","        # If no valid pixels after erosion, return nan\n","        return np.nan\n","\n","    ssim_vals = []\n","    # Calculate SSIM map for each channel, then take mean of core_mask region\n","    for c in range(C):\n","        ref_chan = img_ref[c, :, :]\n","        test_chan = img_test[c, :, :]\n","        try:\n","            # full=True returns (score, ssim_map)\n","            _, ssim_map = ssim(\n","                ref_chan, test_chan,\n","                data_range=max_val,\n","                full=True,\n","                **{k: v for k, v in ssim_kwargs.items()}\n","            )\n","            # ssim_map shape = (H, W). Take average of core_mask region\n","            ssim_vals.append(np.mean(ssim_map[core_mask]))\n","        except Exception as e:\n","            print(f\"[ssim_eroded_mask] Warning: Channel {c} SSIM calculation failed: {e}\")\n","            ssim_vals.append(np.nan)\n","\n","    return float(np.nanmean(ssim_vals))\n","\n","def sam_masked(img_ref, img_test, valid_mask):\n","    \"\"\"\n","    Calculate masked SAM (Spectral Angle Mapper).\n","    img_ref:    numpy array, shape = (C, H, W)\n","    img_test:   numpy array, shape = (C, H, W)\n","    valid_mask: numpy array, shape = (H, W), bool or {0,1}\n","    Returns: a scalar SAM (in degrees), returns np.nan if valid_mask is all zeros.\n","    \"\"\"\n","    if img_ref.shape != img_test.shape:\n","        raise ValueError(\"[sam_masked] Input shapes are inconsistent.\")\n","    C, H, W = img_ref.shape\n","    # Ensure float64 for better precision\n","    I = img_ref.astype(np.float64)\n","    K = img_test.astype(np.float64)\n","\n","    # Find valid pixel indices\n","    mask_bool = valid_mask > 0\n","    if not np.any(mask_bool):\n","        return np.nan\n","\n","    # Calculate spectral angle for each valid pixel\n","    # Shape transformation: flatten (C, H, W) to (C, N_valid)\n","    I_flat = I[:, mask_bool]  # shape = (C, N_valid)\n","    K_flat = K[:, mask_bool]  # shape = (C, N_valid)\n","\n","    # dot product norm\n","    dot = np.sum(I_flat * K_flat, axis=0)  # (N_valid,)\n","    norm_I = np.linalg.norm(I_flat, axis=0)  # (N_valid,)\n","    norm_K = np.linalg.norm(K_flat, axis=0)  # (N_valid,)\n","    # Avoid division by zero\n","    denom = norm_I * norm_K\n","    # Some pixels may be all zeros, causing denom=0, exclude these\n","    valid_idx = denom > 0\n","    if not np.any(valid_idx):\n","        return np.nan\n","\n","    cos_theta = dot[valid_idx] / denom[valid_idx]\n","    # Check numerical range to avoid arccos error\n","    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n","    angles = np.arccos(cos_theta)  # in radians\n","    sam_deg = np.degrees(angles)   # convert to degrees\n","    return float(np.mean(sam_deg))\n","\n","\n","def psnr_masked_gpu_batch(\n","    img_ref: torch.Tensor,   # shape (B, C, H, W)\n","    img_test: torch.Tensor,  # shape (B, C, H, W)\n","    valid_mask: torch.Tensor,# shape (B, 1, H, W) or (B, H, W)\n","    data_range: float = 1.0\n",") -> torch.Tensor:\n","    \"\"\"\n","    Batch Masked PSNR on GPU. Returns a tensor of shape (B,).\n","    \"\"\"\n","    B = img_ref.shape[0]\n","\n","    # Ensure mask shape is (B, 1, H, W)\n","    if valid_mask.dim() == 3:\n","        valid_mask = valid_mask.unsqueeze(1)\n","\n","    # Compute squared error\n","    se = (img_ref - img_test).pow(2)  # (B, C, H, W)\n","\n","    # Count valid pixels per sample\n","    num_pixels = valid_mask.sum(dim=(1, 2, 3))  # (B,)\n","    num_pixels = num_pixels * img_ref.shape[1]  # multiply by channels\n","\n","    # Mask and sum squared errors\n","    se_masked = se * valid_mask  # (B, C, H, W)\n","    mse = se_masked.sum(dim=(1, 2, 3)) / num_pixels.clamp(min=1)  # (B,)\n","\n","    # Compute PSNR\n","    psnr = 10.0 * torch.log10((data_range ** 2) / mse.clamp(min=1e-10))\n","\n","    # Handle invalid samples (no valid pixels)\n","    psnr = torch.where(num_pixels > 0, psnr, torch.tensor(float('nan'), device=img_ref.device))\n","\n","    return psnr  # (B,)\n","\n","def ssim_eroded_mask_gpu_batch(\n","    img_ref: torch.Tensor,\n","    img_test: torch.Tensor,\n","    valid_mask: torch.Tensor,\n","    window_size: int = 3,\n","    data_range: float = 1.0\n",") -> torch.Tensor:\n","    \"\"\"\n","    Corrected version: uses the same Gaussian window parameters as kornia\n","    \"\"\"\n","    B, C, H, W = img_ref.shape\n","\n","    if valid_mask.dim() == 3:\n","        valid_mask = valid_mask.unsqueeze(1)\n","    valid_mask = valid_mask.float()\n","\n","    # SSIM constants\n","    C1 = (0.01 * data_range) ** 2\n","    C2 = (0.03 * data_range) ** 2\n","\n","    # Create Gaussian window - use the same parameters as kornia\n","    sigma = 1.5  # kornia's default value\n","    coords = torch.arange(window_size, device=img_ref.device, dtype=img_ref.dtype)\n","    coords = coords - (window_size - 1) / 2\n","    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n","    g = g / g.sum()\n","    window = g.unsqueeze(0) * g.unsqueeze(1)\n","    window = window.unsqueeze(0).unsqueeze(0)\n","\n","    # Normalize window\n","    window = window / window.sum()\n","\n","    # Expand to all channels\n","    window = window.expand(C, 1, window_size, window_size).contiguous()\n","\n","    # Calculate local statistics\n","    pad = window_size // 2\n","\n","    mu1 = F.conv2d(img_ref, window, padding=pad, groups=C)\n","    mu2 = F.conv2d(img_test, window, padding=pad, groups=C)\n","\n","    mu1_sq = mu1.pow(2)\n","    mu2_sq = mu2.pow(2)\n","    mu1_mu2 = mu1 * mu2\n","\n","    sigma1_sq = F.conv2d(img_ref.pow(2), window, padding=pad, groups=C) - mu1_sq\n","    sigma2_sq = F.conv2d(img_test.pow(2), window, padding=pad, groups=C) - mu2_sq\n","    sigma12 = F.conv2d(img_ref * img_test, window, padding=pad, groups=C) - mu1_mu2\n","\n","    # Calculate SSIM\n","    numerator = (2 * mu1_mu2 + C1) * (2 * sigma12 + C2)\n","    denominator = (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)\n","    ssim_map = numerator / denominator\n","\n","    # Average across channel dimension\n","    ssim_map = ssim_map.mean(dim=1, keepdim=True)\n","\n","    # Erode mask\n","    kernel = torch.ones((window_size, window_size), device=valid_mask.device)\n","    mask_eroded = erosion(valid_mask, kernel, border_type='constant', border_value=0.0)\n","    mask_eroded = (mask_eroded > 0.5).float()\n","\n","    # Calculate masked mean\n","    valid_pixels = mask_eroded.sum(dim=(1, 2, 3))\n","    ssim_masked = ssim_map * mask_eroded\n","    ssim_sum = ssim_masked.sum(dim=(1, 2, 3))\n","\n","    ssim_mean = torch.where(\n","        valid_pixels > 0,\n","        ssim_sum / valid_pixels,\n","        torch.tensor(float('nan'), device=img_ref.device)\n","    )\n","\n","    return ssim_mean\n","\n","def sam_masked_gpu_batch(\n","    img_ref: torch.Tensor,    # (B, C, H, W)\n","    img_pred: torch.Tensor,   # (B, C, H, W)\n","    valid_mask: torch.Tensor, # (B, 1, H, W) or (B, H, W)\n","    eps: float = 1e-8\n",") -> torch.Tensor:\n","    \"\"\"\n","    Batch Masked Spectral Angle Mapper on GPU. Returns a tensor of shape (B,) in degrees.\n","    \"\"\"\n","    B, C, H, W = img_ref.shape\n","\n","    # Ensure mask shape is (B, H, W)\n","    if valid_mask.dim() == 4:\n","        valid_mask = valid_mask.squeeze(1)\n","    elif valid_mask.dim() == 2:\n","        valid_mask = valid_mask.unsqueeze(0).expand(B, -1, -1)\n","\n","    # Reshape for batch processing\n","    img_ref_flat = img_ref.view(B, C, -1)  # (B, C, H*W)\n","    img_pred_flat = img_pred.view(B, C, -1)\n","    mask_flat = valid_mask.view(B, -1) > 0  # (B, H*W)\n","\n","    # Compute spectral angles for all pixels\n","    dot_product = (img_ref_flat * img_pred_flat).sum(dim=1)  # (B, H*W)\n","    norm_ref = img_ref_flat.norm(dim=1)  # (B, H*W)\n","    norm_pred = img_pred_flat.norm(dim=1)\n","\n","    # Avoid division by zero\n","    denominator = (norm_ref * norm_pred).clamp(min=eps)\n","    cos_theta = (dot_product / denominator).clamp(-1.0, 1.0)\n","\n","    # Convert to angles in degrees\n","    angles = torch.acos(cos_theta) * (180.0 / torch.pi)  # (B, H*W)\n","\n","    # Apply mask and compute mean for each sample\n","    sam_mean = torch.zeros(B, device=img_ref.device)\n","    for b in range(B):\n","        valid_angles = angles[b][mask_flat[b]]\n","        if valid_angles.numel() > 0:\n","            sam_mean[b] = valid_angles.mean()\n","        else:\n","            sam_mean[b] = float('nan')\n","\n","    return sam_mean  # (B,)\n","\n","\n","def init_weights(m):\n","    \"\"\"Initialize model weights.\"\"\"\n","    if isinstance(m, (nn.Conv2d, nn.Linear)):\n","        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n","        if m.bias is not None:\n","            nn.init.zeros_(m.bias)\n","# -------------------------------------------------------------------------\n","# 3. Parse command‐line arguments (modify this block)\n","# -------------------------------------------------------------------------\n","def parse_args(argv=None):\n","    parser = argparse.ArgumentParser(\n","        description=\"Unified training script: dynamically load model and loss classes, then train/val/test.\"\n","    )\n","\n","    # Add a new --mode flag:\n","    parser.add_argument(\n","        \"--mode\",\n","        type=str,\n","        choices=[\"train\", \"test\"],\n","        default=\"train\",\n","        help=\"Choose 'train' to train from scratch (or resume), or 'test' to only run evaluation on a saved checkpoint.\"\n","    )\n","\n","    # Model & loss\n","    parser.add_argument(\n","        \"--model\", type=str, required=True,\n","        help=\"Model name, matching a file in models/ that defines MODEL_CLASS.\"\n","    )\n","    parser.add_argument(\n","        \"--loss\", type=str, required=True,\n","        help=\"Loss name, matching a file in losses/ that defines LOSS_CLASS.\"\n","    )\n","\n","    parser.add_argument(\n","        \"--data-dir\", type=str, default=\"data\",\n","        help=\"Root data directory (must contain train/, val/, test/ subfolders).\"\n","    )\n","    parser.add_argument(\n","        \"--batch-size\", type=int, default=32, help=\"Batch size for train/val/test.\"\n","    )\n","    parser.add_argument(\n","        \"--epochs\", type=int, default=15, help=\"Total number of training epochs.\"\n","    )\n","    parser.add_argument(\n","        \"--lr\", type=float, default=1e-4, help=\"Initial learning rate.\"\n","    )\n","    parser.add_argument(\n","        \"--weight-decay\", type=float, default=1e-5, help=\"Weight decay (L2) coefficient.\"\n","    )\n","    parser.add_argument(\n","        \"--use-amp\", action=\"store_true\",\n","        help=\"If set, enable PyTorch AMP (automatic mixed precision).\"\n","    )\n","    parser.add_argument(\n","        \"--gpu\", type=int, default=0,\n","        help=\"GPU id to use; set to -1 for CPU.\"\n","    )\n","\n","    parser.add_argument(\n","        \"--ckpt-dir\", type=str, default=\"checkpoints\",\n","        help=\"Directory for saving checkpoints.\"\n","    )\n","    parser.add_argument(\n","        \"--log-dir\", type=str, default=\"logs\",\n","        help=\"Directory for saving logs and intermediate results.\"\n","    )\n","    parser.add_argument(\n","        \"--plot-dir\", type=str, default=\"plots\",\n","        help=\"Directory for saving plots.\"\n","    )\n","    parser.add_argument(\n","        \"--resume\", type=str, default=None,\n","        help=\"If specified, load this checkpoint and resume training or do test (depending on --mode).\"\n","    )\n","\n","    return parser.parse_args(argv)"]},{"cell_type":"markdown","metadata":{"id":"78XZXEqwFWii"},"source":["# Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuTlzT_g-RGo"},"outputs":[],"source":["def main(args):\n","    # Device selection\n","    if args.gpu >= 0 and torch.cuda.is_available():\n","        device = torch.device(f\"cuda:{args.gpu}\")\n","    else:\n","        device = torch.device(\"cpu\")\n","    print(f\"[INFO] Using device: {device}\")\n","\n","    # Create checkpoint and log directories\n","    ckpt_dir = Path(args.ckpt_dir)\n","    ckpt_dir.mkdir(parents=True, exist_ok=True)\n","    log_dir = Path(args.log_dir)\n","    log_dir.mkdir(parents=True, exist_ok=True)\n","    plot_dir = Path(args.plot_dir)\n","    plot_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Instantiate model, loss, optimizer, scheduler\n","    model = Unet(use_meta=False, use_selfattention=True,use_spatial_attention=True)#True,False\n","    model = model.to(device)\n","\n","    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"[INFO] Loaded model `{args.model}`, trainable params = {num_params:,}\")\n","    criterion = MSELoss()\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","\n","    use_amp = args.use_amp and (device.type.startswith(\"cuda\"))\n","    # scaler = torch.cuda.amp.GradScaler() if use_amp else None\n","    scaler = torch.amp.GradScaler() if use_amp else None\n","    if use_amp:\n","        print(\"[INFO] Enabled AMP mixed‐precision.\")\n","\n","    # 5) Prepare datasets and dataloaders\n","    train_dir = Path(args.data_dir) / \"train\"\n","    val_dir   = Path(args.data_dir) / \"val\"\n","    test_dir  = Path(args.data_dir) / \"test\"\n","    for d in (train_dir, val_dir, test_dir):\n","        if not d.exists() or not d.is_dir():\n","            raise RuntimeError(f\"[ERROR] Directory `{d}` does not exist or is not a folder.\")\n","    print(\"[INFO] train dataset creating\")\n","    train_dataset = OptimizedPatchDataset(train_dir, cache_size=500, map_location=\"cpu\")\n","    print(\"[INFO] train dataset created\")\n","    val_dataset   = OptimizedPatchDataset(val_dir,   cache_size=500, map_location=\"cpu\")\n","    print(\"[INFO] val dataset created\")\n","    test_dataset  = OptimizedPatchDataset(test_dir,  cache_size=500, map_location=\"cpu\")\n","    print(\"[INFO] test dataset created\")\n","\n","    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n","    print(\"[INFO] train dataset loaded\")\n","    val_loader   = DataLoader(val_dataset,   batch_size=args.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n","    test_loader  = DataLoader(test_dataset,  batch_size=args.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n","    print(f\"[INFO] #train = {len(train_dataset)}, #val = {len(val_dataset)}, #test = {len(test_dataset)}\")\n","\n","    # Learning rate schedulers\n","    total_iteration = (len(train_dataset) // args.batch_size)*args.epochs\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_iteration, eta_min=1e-6)\n","\n","    # 6) If resume is specified, load checkpoint (for either training or testing)\n","    start_epoch   = 0\n","    best_val_loss = float(\"inf\")\n","    if args.resume is not None:\n","        if os.path.isfile(args.resume):\n","            ckpt = torch.load(args.resume, map_location=device,)\n","            model.load_state_dict(ckpt[\"model_state_dict\"])\n","            optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n","            scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n","            start_epoch   = ckpt.get(\"epoch\", 0)\n","            best_val_loss = ckpt.get(\"best_val_loss\", best_val_loss)\n","            print(f\"[INFO] Loaded checkpoint `{args.resume}`, starting epoch = {start_epoch}, best_val_loss = {best_val_loss:.6f}\")\n","        else:\n","            print(f\"[WARN] Specified resume file `{args.resume}` not found, ignoring.\")\n","\n","    # 7) Define train_one_epoch, validate_one_epoch, test_and_evaluate (unchanged) …\n","    def train_one_epoch(epoch_idx):\n","        model.train()\n","        running_loss = 0.0\n","        num_batches = 0\n","        pbar = tqdm(train_loader, desc=f\"Epoch {epoch_idx+1}/{args.epochs} [Train]\")\n","        for batch in pbar:\n","            # Move inputs to device\n","            l30_img     = batch[\"l30_img\"].to(device, non_blocking=True)\n","            # mask_l30    = batch[\"mask_l30\"].to(device, non_blocking=True)\n","            l30_meta    = batch[\"l30_meta\"].to(device, non_blocking=True)\n","\n","            s1_img      = batch[\"s1_img\"].to(device, non_blocking=True)\n","            # mask_s1     = batch[\"mask_s1\"].to(device, non_blocking=True)\n","            s1_meta     = batch[\"s1_meta\"].to(device, non_blocking=True)\n","\n","            planet_img  = batch[\"planet_img\"].to(device, non_blocking=True)\n","            # mask_planet = batch[\"mask_planet\"].to(device, non_blocking=True)\n","            planet_meta = batch[\"planet_meta\"].to(device, non_blocking=True)\n","\n","            s30_gt      = batch[\"s30_img_gt\"].to(device, non_blocking=True)\n","            mask_s30    = batch[\"mask_s30\"].to(device, non_blocking=True)\n","\n","            optimizer.zero_grad()\n","            if use_amp:\n","                with torch.amp.autocast('cuda'):\n","                    # outputs = model(\n","                    #     l30_img, mask_l30, l30_meta,\n","                    #     s1_img, mask_s1, s1_meta,\n","                    #     planet_img, mask_planet, planet_meta\n","                    # )\n","                    outputs = model(\n","                        l30_img, l30_meta,\n","                        s1_img, s1_meta,\n","                        planet_img, planet_meta\n","                    )\n","                    loss = criterion(outputs, s30_gt, mask_s30)\n","                    ssims=ssim_eroded_mask_gpu_batch(s30_gt, outputs, mask_s30).mean()\n","                    sams=sam_masked_gpu_batch(s30_gt, outputs, mask_s30).mean()\n","                    psnrs=psnr_masked_gpu_batch(s30_gt, outputs, mask_s30).mean()\n","\n","                    loss_ssim  = (1 - ssims)   # 结构损失\n","                    loss_sam   = sams/180         # 光谱角度损失\n","                    loss_psnr  = 1.0 - psnrs/50.0\n","                    loss = 1*loss + 0.5*loss_ssim + 0.5*loss_sam + 0.2 * loss_psnr\n","\n","                scaler.scale(loss).backward()\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                scaler.step(optimizer)\n","                scaler.update()\n","            else:\n","                # outputs = model(\n","                #     l30_img, mask_l30, l30_meta,\n","                #     s1_img, mask_s1, s1_meta,\n","                #     planet_img, mask_planet, planet_meta\n","                # )\n","                outputs = model(\n","                        l30_img, l30_meta,\n","                        s1_img, s1_meta,\n","                        planet_img, planet_meta\n","                )\n","                loss = criterion(outputs, s30_gt, mask_s30)\n","\n","                ssims=ssim_eroded_mask_gpu_batch(s30_gt, outputs, mask_s30).mean()\n","                sams=sam_masked_gpu_batch(s30_gt, outputs, mask_s30).mean()\n","                psnrs=psnr_masked_gpu_batch(s30_gt, outputs, mask_s30).mean()\n","\n","                loss_ssim  = (1 - ssims)   # 结构损失\n","                loss_sam   = sams/180         # 光谱角度损失\n","                loss_psnr  = 1.0 - psnrs/50.0\n","                loss = 1*loss + 0.5*loss_ssim + 0.5*loss_sam + 0.2 * loss_psnr\n","\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                optimizer.step()\n","            scheduler.step()\n","            running_loss += loss.item()\n","            num_batches += 1\n","            pbar.set_postfix({\"loss\": f\"{loss.item():.4e}\"})\n","\n","        return running_loss / max(num_batches, 1)\n","\n","    def validate_one_epoch():\n","        model.eval()\n","        val_loss_sum = 0.0\n","        num_batches = 0\n","        with torch.no_grad():\n","            pbar = tqdm(val_loader, desc=\"[Validate]\")\n","            for batch in pbar:\n","                # Move to device ...\n","                l30_img     = batch[\"l30_img\"].to(device, non_blocking=True)\n","                # mask_l30    = batch[\"mask_l30\"].to(device, non_blocking=True)\n","                l30_meta    = batch[\"l30_meta\"].to(device, non_blocking=True)\n","\n","                s1_img      = batch[\"s1_img\"].to(device, non_blocking=True)\n","                # mask_s1     = batch[\"mask_s1\"].to(device, non_blocking=True)\n","                s1_meta     = batch[\"s1_meta\"].to(device, non_blocking=True)\n","\n","                planet_img  = batch[\"planet_img\"].to(device, non_blocking=True)\n","                # mask_planet = batch[\"mask_planet\"].to(device, non_blocking=True)\n","                planet_meta = batch[\"planet_meta\"].to(device, non_blocking=True)\n","\n","                s30_gt      = batch[\"s30_img_gt\"].to(device, non_blocking=True)\n","                mask_s30    = batch[\"mask_s30\"].to(device, non_blocking=True)\n","\n","                # outputs = model(\n","                #     l30_img, mask_l30, l30_meta,\n","                #     s1_img, mask_s1, s1_meta,\n","                #     planet_img, mask_planet, planet_meta\n","                # )\n","                outputs = model(\n","                        l30_img, l30_meta,\n","                        s1_img, s1_meta,\n","                        planet_img, planet_meta\n","                )\n","                loss = criterion(outputs, s30_gt, mask_s30)\n","\n","                ssims=ssim_eroded_mask_gpu_batch(s30_gt, outputs, mask_s30).mean()\n","                sams=sam_masked_gpu_batch(s30_gt, outputs, mask_s30).mean()\n","                psnrs=psnr_masked_gpu_batch(s30_gt, outputs, mask_s30).mean()\n","\n","                loss_ssim  = (1 - ssims)   # 结构损失\n","                loss_sam   = sams/180         # 光谱角度损失\n","                loss_psnr  = 1.0 - psnrs/50.0\n","                loss = 1*loss + 0.5*loss_ssim + 0.5*loss_sam + 0.2 * loss_psnr\n","\n","                val_loss_sum += loss.item()\n","                num_batches += 1\n","                pbar.set_postfix({\"val_loss\": f\"{loss.item():.4e}\"})\n","\n","        return val_loss_sum / max(num_batches, 1)\n","\n","    def test_and_evaluate():\n","        \"\"\"\n","        Run full test‐set evaluation (masked MSE/RMSE/PSNR/SSIM/SAM), save results.\n","        \"\"\"\n","        model.eval()\n","        mse_list, rmse_list, psnr_list, ssim_list, sam_list = [], [], [], [], []\n","        num_valid_samples = 0\n","        total_valid_pixels = 0\n","\n","        save_sample_vis = True\n","        num_samples_to_save = 10\n","        saved_cnt = 0\n","        sample_vis_dir = log_dir / \"sample_visuals\"\n","        if save_sample_vis:\n","            sample_vis_dir.mkdir(parents=True, exist_ok=True)\n","\n","        with torch.no_grad():\n","            pbar = tqdm(test_loader, desc=\"[Test]\")\n","            for batch_idx, batch in enumerate(pbar):\n","                # Move to device ...\n","                l30_img     = batch[\"l30_img\"].to(device, non_blocking=True)\n","                # mask_l30    = batch[\"mask_l30\"].to(device, non_blocking=True)\n","                l30_meta    = batch[\"l30_meta\"].to(device, non_blocking=True)\n","\n","                s1_img      = batch[\"s1_img\"].to(device, non_blocking=True)\n","                # mask_s1     = batch[\"mask_s1\"].to(device, non_blocking=True)\n","                s1_meta     = batch[\"s1_meta\"].to(device, non_blocking=True)\n","\n","                planet_img  = batch[\"planet_img\"].to(device, non_blocking=True)\n","                # mask_planet = batch[\"mask_planet\"].to(device, non_blocking=True)\n","                planet_meta = batch[\"planet_meta\"].to(device, non_blocking=True)\n","\n","                s30_gt      = batch[\"s30_img_gt\"].to(device, non_blocking=True)\n","                mask_s30    = batch[\"mask_s30\"].to(device, non_blocking=True)\n","\n","                # outputs = model(\n","                #     l30_img, mask_l30, l30_meta,\n","                #     s1_img, mask_s1, s1_meta,\n","                #     planet_img, mask_planet, planet_meta\n","                # )\n","                outputs = model(\n","                        l30_img, l30_meta,\n","                        s1_img, s1_meta,\n","                        planet_img, planet_meta\n","                )\n","                mask_ext = mask_s30.expand(-1, outputs.size(1), -1, -1)\n","                outputs = outputs * mask_ext\n","\n","                pred_np = outputs.detach().cpu().numpy()    # (B, C, H, W)\n","                gt_np   = s30_gt.detach().cpu().numpy()     # (B, C, H, W)\n","                mask_np = mask_s30.detach().cpu().numpy().squeeze(1)  # (B, H, W)\n","\n","                B = pred_np.shape[0]\n","                for i in range(B):\n","                    pred_i = pred_np[i]\n","                    gt_i   = gt_np[i]\n","                    mask_i = mask_np[i]\n","\n","                    # Masked MSE / RMSE\n","                    mse_map = (pred_i - gt_i) ** 2  # (C, H, W)\n","                    num_valid_pixels = np.sum(mask_i) * pred_i.shape[0]\n","                    if num_valid_pixels > 0:\n","                        mse_val = np.sum(mse_map * mask_i[np.newaxis, ...]) / num_valid_pixels\n","                    else:\n","                        mse_val = np.nan\n","                    rmse_val = np.sqrt(mse_val) if not np.isnan(mse_val) else np.nan\n","\n","                    # Masked PSNR\n","                    psnr_val = psnr_masked(gt_i, pred_i, mask_i, data_range=1.0)\n","\n","                    # Masked & eroded SSIM\n","                    ssim_val = ssim_eroded_mask(gt_i, pred_i, mask_i, max_val=1.0)\n","\n","                    # Masked SAM\n","                    sam_val = sam_masked(gt_i, pred_i, mask_i)\n","\n","                    if not np.isnan(mse_val):\n","                        mse_list.append(mse_val)\n","                        rmse_list.append(rmse_val)\n","                        psnr_list.append(psnr_val)\n","                        ssim_list.append(ssim_val)\n","                        sam_list.append(sam_val)\n","                        num_valid_samples += 1\n","                        total_valid_pixels += int(np.sum(mask_i))\n","\n","                    # Visualization: save first few samples as RGB comparison\n","                    if save_sample_vis and saved_cnt < num_samples_to_save:\n","                        import matplotlib.pyplot as plt\n","\n","                        if pred_i.shape[0] >= 3:\n","                            pred_rgb = np.clip(pred_i[:3].transpose(1, 2, 0), 0, 1)\n","                            gt_rgb   = np.clip(gt_i[:3].transpose(1, 2, 0), 0, 1)\n","                        else:\n","                            pred_rgb = np.repeat(pred_i[0:1].transpose(1, 2, 0), 3, axis=2)\n","                            gt_rgb   = np.repeat(gt_i[0:1].transpose(1, 2, 0), 3, axis=2)\n","\n","                        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","                        axes[0].imshow(gt_rgb)\n","                        axes[0].set_title(\"GT (RGB)\")\n","                        axes[0].axis(\"off\")\n","                        axes[1].imshow(pred_rgb)\n","                        axes[1].set_title(\"Pred (RGB)\")\n","                        axes[1].axis(\"off\")\n","                        plt.tight_layout()\n","                        vis_path = sample_vis_dir / f\"sample_{batch_idx:04d}_{i:02d}.png\"\n","                        plt.savefig(vis_path)\n","                        plt.close(fig)\n","                        saved_cnt += 1\n","\n","                if mse_list:\n","                    pbar.set_postfix({\n","                        \"MSE\":  f\"{np.nanmean(mse_list):.6f}\",\n","                        \"RMSE\": f\"{np.nanmean(rmse_list):.6f}\",\n","                        \"PSNR\": f\"{np.nanmean(psnr_list):.4f}\"\n","                    })\n","\n","        # Compute means and stds\n","        mse_arr  = np.array(mse_list)\n","        rmse_arr = np.array(rmse_list)\n","        psnr_arr = np.array(psnr_list)\n","        ssim_arr = np.array(ssim_list)\n","        sam_arr  = np.array(sam_list)\n","\n","        results = {\n","            \"mse_mean\": float(np.nanmean(mse_arr)),\n","            \"mse_std\":  float(np.nanstd(mse_arr)),\n","            \"rmse_mean\": float(np.nanmean(rmse_arr)),\n","            \"rmse_std\":  float(np.nanstd(rmse_arr)),\n","            \"psnr_mean\": float(np.nanmean(psnr_arr)),\n","            \"psnr_std\":  float(np.nanstd(psnr_arr)),\n","            \"ssim_mean\": float(np.nanmean(ssim_arr)),\n","            \"ssim_std\":  float(np.nanstd(ssim_arr)),\n","            \"sam_mean\":  float(np.nanmean(sam_arr)),\n","            \"sam_std\":   float(np.nanstd(sam_arr)),\n","            \"num_valid_samples\": num_valid_samples,\n","            \"total_valid_pixels\": total_valid_pixels,\n","        }\n","\n","        results_file = log_dir / \"test_results.txt\"\n","        with open(results_file, \"w\") as f:\n","            f.write(\"===== Test Set Evaluation =====\\n\")\n","            f.write(f\"MSE (Masked)   : {results['mse_mean']:.6f} ± {results['mse_std']:.6f}\\n\")\n","            f.write(f\"RMSE (Masked)  : {results['rmse_mean']:.6f} ± {results['rmse_std']:.6f}\\n\")\n","            f.write(f\"PSNR (Masked)  : {results['psnr_mean']:.4f} ± {results['psnr_std']:.4f} dB\\n\")\n","            f.write(f\"SSIM (Masked)  : {results['ssim_mean']:.4f} ± {results['ssim_std']:.4f}\\n\")\n","            f.write(f\"SAM (Masked)   : {results['sam_mean']:.4f} ± {results['sam_std']:.4f} degrees\\n\")\n","            f.write(f\"Valid Samples  : {results['num_valid_samples']}\\n\")\n","            f.write(f\"Valid Pixels   : {results['total_valid_pixels']:,}\\n\")\n","        print(f\"[INFO] Test results saved to {results_file}\")\n","        print(results)\n","        return results\n","\n","    # -------------------------------------------------------------------------\n","    # 8) If mode == \"test\", skip training loop and just run evaluation\n","    # -------------------------------------------------------------------------\n","    if args.mode == \"test\":\n","        print(\"[INFO] Mode = TEST → skipping training, running only test/evaluation.\")\n","        # If resume was provided, the checkpoint was already loaded above.\n","        # If resume is None, we cannot test—raise an error.\n","        if args.resume is None:\n","            raise RuntimeError(\"[ERROR] --mode test requires --resume <checkpoint_path> to be specified.\")\n","        # Run test_and_evaluate() and then exit\n","        test_results = test_and_evaluate()\n","        print(\"[INFO] Done with test/evaluation. Exiting.\")\n","        return\n","\n","    # -------------------------------------------------------------------------\n","    # 9) Otherwise (mode == \"train\"), run the full train/val/test loop\n","    # -------------------------------------------------------------------------\n","    train_losses=[]\n","    val_losses=[]\n","    for epoch in range(start_epoch, args.epochs):\n","        epoch_start_time = time.time()\n","\n","        train_loss = train_one_epoch(epoch)\n","        train_losses.append(train_loss)\n","        val_loss   = validate_one_epoch()\n","        val_losses.append(val_loss)\n","\n","        # scheduler.step(val_loss)\n","\n","        elapsed = time.time() - epoch_start_time\n","        print(\n","            f\"[Epoch {epoch+1}/{args.epochs}] \"\n","            f\"Train Loss: {train_loss:.6f}  Val Loss: {val_loss:.6f}  \"\n","            f\"Time: {elapsed:.1f}s\"\n","        )\n","\n","        # Save best checkpoint\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            ckpt_path = ckpt_dir / f\"best_{args.model}_epoch{epoch+1}_valloss{val_loss:.4f}.pth\"\n","            torch.save({\n","                \"epoch\": epoch + 1,\n","                \"model_state_dict\": model.state_dict(),\n","                \"optimizer_state_dict\": optimizer.state_dict(),\n","                \"scheduler_state_dict\": scheduler.state_dict(),\n","                \"best_val_loss\": best_val_loss\n","            }, ckpt_path)\n","            print(f\"[INFO] Validation loss improved → saved checkpoint to {ckpt_path}\")\n","\n","    print(f\"[INFO] Training finished, best val loss = {best_val_loss:.6f}\")\n","\n","    # Finally run full test/evaluation once more\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"[INFO] Running final test/evaluation …\")\n","    print(\"=\"*60)\n","    _ = test_and_evaluate()\n","    print(\"[INFO] Test evaluation completed.\\n\")\n","    #%% --- Plot Training Curves ---\n","    plt.figure(figsize=(12, 5))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue')\n","    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='red')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss (MSE - Masked)')\n","    plt.title('Training and Validation Loss')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', color='blue')\n","    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='red')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss (Log Scale)')\n","    plt.title('Training and Validation Loss (Log Scale)')\n","    plt.yscale('log')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plot_path = plot_dir / \"loss_curves.png\"\n","    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n","    print(f\"Saved loss curves to {plot_path}\")\n"]},{"cell_type":"markdown","metadata":{"id":"GdiyMiI6Ow9M"},"source":["# Copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgbKwRxFOzbk"},"outputs":[],"source":["import shutil\n","import os\n","\n","src = \"/content/drive/MyDrive/ColabData/ReconstructionDataset_Final\"\n","dst = \"/content/data/ReconstructionDataset_Final\"\n","\n","if os.path.exists(dst):\n","    shutil.rmtree(dst)\n","shutil.copytree(src, dst)\n","\n","print(\"Copied files:\", os.listdir(dst))"]},{"cell_type":"markdown","metadata":{"id":"EjWHUg8JXbDD"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEBBS1UcXcrn"},"outputs":[],"source":["class IDEArgs: pass\n","args = IDEArgs()\n","args.mode        = \"train\"\n","args.model       = \"MultimodalUnetadap\"\n","args.loss        = \"mse_loss\"\n","args.data_dir    = \"/content/data/ReconstructionDataset_Final\"\n","args.batch_size  = 64\n","args.epochs      = 300\n","args.lr          = 1e-4\n","args.weight_decay= 1e-5\n","args.use_amp     = True#False\n","args.gpu         = 0\n","args.ckpt_dir    = \"/content/drive/MyDrive/Colabdata/checkpoints/\"+args.model\n","args.log_dir     = \"/content/drive/MyDrive/Colabdata/logs/\"+args.model\n","args.plot_dir     = \"/content/drive/MyDrive/Colabdata/plots/\"+args.model\n","args.resume      = None\n","main(args)"]},{"cell_type":"markdown","metadata":{"id":"q2fi0mAeOG7l"},"source":["#infer-import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXFQhnRCOIic"},"outputs":[],"source":["!pip install rasterio\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Replace with the actual path to the directory containing model2.py\n","model_dir_in_drive = '/content/drive/MyDrive/ColabModel/'\n","loss_dir_in_drive = '/content/drive/MyDrive/ColabLoss/'\n","\n","import sys\n","# Add the directory to the Python path\n","if model_dir_in_drive not in sys.path:\n","    sys.path.append(model_dir_in_drive)\n","    print(f\"Added {model_dir_in_drive} to sys.path\")\n","else:\n","    print(f\"{model_dir_in_drive} already in sys.path\")\n","sys.path.append(loss_dir_in_drive)\n","\n","from MultimodalUnetnewadap import Unet\n","import os\n","import argparse\n","from pathlib import Path\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from collections import defaultdict\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.ndimage import binary_erosion\n","from skimage.metrics import structural_similarity as ssim\n","import importlib\n","\n","import rasterio\n","from rasterio.transform import Affine\n","from rasterio.errors import NotGeoreferencedWarning\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)"]},{"cell_type":"markdown","metadata":{"id":"eCxgr1UZOOZ-"},"source":["# infer- config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PrGtJtjIONyt"},"outputs":[],"source":["# -------------------------------------------------------------\n","# 1. Define dataset class (consistent with OptimizedPatchDataset in training script)\n","# -------------------------------------------------------------\n","class OptimizedPatchDataset(Dataset):\n","    \"\"\"Dataset class for loading .pt files with caching,\n","    automatically removing samples whose L30, S1, Planet mask all equal zero.\n","    \"\"\"\n","    def __init__(self, data_dir, cache_size=50, map_location=\"cpu\"):\n","        self.data_dir = Path(data_dir)\n","        all_files = sorted(self.data_dir.glob(\"sample_*.pt\"))\n","        if not all_files:\n","            raise RuntimeError(f\"[OptimizedPatchDataset] No sample_*.pt files found in {self.data_dir}\")\n","\n","        # Filter: only keep samples with at least one non-zero mask\n","        valid_files = []\n","        for p in all_files:\n","            sample = torch.load(p, map_location=map_location)\n","            m0 = sample.get('mask_l30')\n","            m1 = sample.get('mask_s1')\n","            m2 = sample.get('mask_planet')\n","            # Keep if any mask is not all zeros\n","            if not ((m0.sum()==0) and (m1.sum()==0) and (m2.sum()==0)):\n","                valid_files.append(p)\n","        if not valid_files:\n","            raise RuntimeError(\"[OptimizedPatchDataset] All samples have zero masks!\")\n","\n","        self.files = valid_files\n","        self.cache = {}\n","        self.cache_size = cache_size\n","        self.map_location = map_location\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self, idx):\n","        if idx in self.cache:\n","            return self.cache[idx]\n","        sample = torch.load(self.files[idx], map_location=self.map_location)\n","        # Type adjustment & contiguous\n","        for key, value in sample.items():\n","            if isinstance(value, torch.Tensor):\n","                if value.dtype == torch.float64:\n","                    value = value.float()\n","                sample[key] = value.contiguous()\n","        # Cache\n","        sample[\"__path__\"] = str(self.files[idx])\n","        if len(self.cache) < self.cache_size:\n","            self.cache[idx] = sample\n","        return sample\n","\n","# -------------------------------------------------------------\n","# 2. Define metric functions (same version as used in training)\n","# -------------------------------------------------------------\n","\n","def psnr_masked(img_ref, img_test, valid_mask, data_range=1.0):\n","    \"\"\"Calculate masked PSNR\"\"\"\n","    if img_ref.shape != img_test.shape:\n","        raise ValueError(\"[psnr_masked] Shape mismatch\")\n","    if valid_mask.shape != img_ref.shape[1:]:\n","        raise ValueError(f\"[psnr_masked] Mask shape mismatch\")\n","    I = img_ref.astype(np.float64)\n","    K = img_test.astype(np.float64)\n","    num_valid = np.sum(valid_mask)\n","    if num_valid == 0:\n","        return float(\"nan\")\n","    sq_err = (I - K) ** 2\n","    mask_chw = np.expand_dims(valid_mask, axis=0).repeat(I.shape[0], axis=0)\n","    masked_sq = sq_err[mask_chw > 0]\n","    mse = np.sum(masked_sq) / (num_valid * I.shape[0])\n","    if mse <= 0:\n","        return float(\"inf\")\n","    psnr_val = 10.0 * np.log10((data_range ** 2) / mse)\n","    return float(psnr_val)\n","\n","def ssim_eroded_mask(img_ref, img_test, valid_mask, max_val=1.0, **ssim_kwargs):\n","    \"\"\"Calculate SSIM with eroded mask\"\"\"\n","    if img_ref.shape != img_test.shape:\n","        raise ValueError(\"[ssim_eroded_mask] Shape mismatch\")\n","    C, H, W = img_ref.shape\n","    orig = valid_mask > 0\n","    win_size = ssim_kwargs.pop('win_size', min(3, H, W))\n","    if win_size % 2 == 0:\n","        win_size -= 1\n","    if win_size < 3:\n","        return float(\"nan\")\n","    struct_el = np.ones((win_size, win_size), dtype=bool)\n","    core = binary_erosion(orig, structure=struct_el, border_value=0)\n","    if np.count_nonzero(core) == 0:\n","        return float(\"nan\")\n","    ssim_vals = []\n","    for c in range(C):\n","        ref_c = img_ref[c, :, :]\n","        test_c = img_test[c, :, :]\n","        try:\n","            _, ssim_map = ssim(\n","                ref_c, test_c,\n","                data_range=max_val,\n","                full=True,\n","                **ssim_kwargs\n","            )\n","            ssim_vals.append(np.mean(ssim_map[core]))\n","        except Exception:\n","            ssim_vals.append(float(\"nan\"))\n","    return float(np.nanmean(ssim_vals))\n","\n","def sam_masked(img_ref, img_test, valid_mask):\n","    \"\"\"Calculate masked SAM (Spectral Angle Mapper)\"\"\"\n","    if img_ref.shape != img_test.shape:\n","        raise ValueError(\"[sam_masked] Shape mismatch\")\n","    C, H, W = img_ref.shape\n","    I = img_ref.astype(np.float64)\n","    K = img_test.astype(np.float64)\n","    mask_bool = valid_mask > 0\n","    if not np.any(mask_bool):\n","        return float(\"nan\")\n","    I_flat = I[:, mask_bool]\n","    K_flat = K[:, mask_bool]\n","    dot = np.sum(I_flat * K_flat, axis=0)\n","    norm_I = np.linalg.norm(I_flat, axis=0)\n","    norm_K = np.linalg.norm(K_flat, axis=0)\n","    denom = norm_I * norm_K\n","    valid_idx = denom > 0\n","    if not np.any(valid_idx):\n","        return float(\"nan\")\n","    cos_theta = dot[valid_idx] / denom[valid_idx]\n","    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n","    angles = np.arccos(cos_theta)\n","    sam_deg = np.degrees(angles)\n","    return float(np.mean(sam_deg))\n","\n","def save_multiband_tif_rasterio(array_3d, output_path):\n","    \"\"\"\n","    Use rasterio to save array_3d with shape (C, H, W) as a 12-band GeoTIFF (without geographic coordinate information)\n","    \"\"\"\n","    C, H, W = array_3d.shape\n","    assert C <= 2**16, \"Number of bands should not be too large!\"\n","    # No geographic reference, Affine.identity is equivalent to pixel-coordinate identity matrix\n","    transform = Affine.identity()\n","\n","    # Open a new write handle\n","    with rasterio.open(\n","        output_path,\n","        'w',\n","        driver='GTiff',\n","        height=H,\n","        width=W,\n","        count=C,            # Number of bands\n","        dtype=array_3d.dtype,\n","        crs=None,           # No coordinate system\n","        transform=transform\n","    ) as dst:\n","        # Write band by band\n","        for band_idx in range(C):\n","            dst.write(array_3d[band_idx], band_idx + 1)"]},{"cell_type":"markdown","metadata":{"id":"s0wrWCh3Vv0L"},"source":["# infer-main\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCKetC91V01G"},"outputs":[],"source":["# -------------------------------------------------------------\n","# 4. Inference main function\n","# -------------------------------------------------------------\n","def main(args):\n","    # Select device\n","    device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n","    print(f\"[INFO] Using device: {device}\")\n","\n","    # Create output directories\n","    output_dir = Path(args.output_dir)\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","    vis_dir = output_dir / \"visualizations\"\n","    vis_dir.mkdir(parents=True, exist_ok=True)\n","    arrays_dir = output_dir / \"arrays\"\n","    arrays_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # ------------------------------------------------------------------\n","    # 4.1 Load model\n","    # ------------------------------------------------------------------\n","    model = Unet(use_meta=False, use_selfattention=True,use_spatial_attention=True).to(device)#True,False\n","    # Load checkpoint\n","    if not Path(args.ckpt_path).is_file():\n","        raise FileNotFoundError(f\"[ERROR] Checkpoint not found: {args.ckpt_path}\")\n","    ckpt = torch.load(args.ckpt_path, map_location=device,)\n","    if \"model_state_dict\" in ckpt:\n","        model.load_state_dict(ckpt[\"model_state_dict\"])\n","    else:\n","        model.load_state_dict(ckpt)\n","\n","    model.eval()\n","    print(f\"[INFO] Loaded checkpoint from {args.ckpt_path}\")\n","\n","    # ------------------------------------------------------------------\n","    # 4.2 Prepare test dataset DataLoader\n","    # ------------------------------------------------------------------\n","    test_dir = Path(args.data_dir) / \"test\"\n","    if not test_dir.exists():\n","        raise RuntimeError(f\"[ERROR] Test directory '{test_dir}' does not exist\")\n","    test_dataset = OptimizedPatchDataset(test_dir, cache_size=100, map_location=\"cpu\")\n","    test_loader = DataLoader(\n","        test_dataset,\n","        batch_size=args.batch_size,\n","        shuffle=False,\n","        num_workers=args.num_workers,\n","        pin_memory=True\n","    )\n","\n","    # ------------------------------------------------------------------\n","    # 4.3 Inference loop: run inference on all samples in test_loader and calculate metrics\n","    # ------------------------------------------------------------------\n","    results = []\n","    total_mse, total_rmse, total_psnr, total_ssim, total_sam = [], [], [], [], []\n","    metrics_by_combo = defaultdict(list)\n","\n","    with torch.no_grad():\n","        pbar = tqdm(test_loader, desc=\"[Inference]\")\n","        for batch_idx, batch in enumerate(pbar):\n","            # Move data to device\n","            l30_img = batch[\"l30_img\"].to(device, non_blocking=True)\n","            mask_l30 = batch[\"mask_l30\"].to(device, non_blocking=True)\n","            l30_meta = batch[\"l30_meta\"].to(device, non_blocking=True)\n","            s1_img   = batch[\"s1_img\"].to(device, non_blocking=True)\n","            mask_s1  = batch[\"mask_s1\"].to(device, non_blocking=True)\n","            s1_meta  = batch[\"s1_meta\"].to(device, non_blocking=True)\n","            planet_img = batch[\"planet_img\"].to(device, non_blocking=True)\n","            mask_planet = batch[\"mask_planet\"].to(device, non_blocking=True)\n","            planet_meta = batch[\"planet_meta\"].to(device, non_blocking=True)\n","            s30_gt   = batch[\"s30_img_gt\"].to(device, non_blocking=True)\n","            mask_s30 = batch[\"mask_s30\"].to(device, non_blocking=True)\n","\n","            # Forward inference\n","            fake_s30 = model(\n","                l30_img, l30_meta,\n","                s1_img, s1_meta,\n","                planet_img, planet_meta\n","            )\n","\n","            mask_ext = mask_s30.expand(-1, fake_s30.size(1), -1, -1)\n","            fake_s30 = fake_s30 * mask_ext\n","\n","            # Convert to numpy array for metric calculation and saving\n","            fake_np = fake_s30.detach().cpu().numpy()  # shape (B, C=12, H, W)\n","            gt_np   = s30_gt.detach().cpu().numpy()    # shape (B, C=12, H, W)\n","            mask_np = mask_s30.detach().cpu().numpy().squeeze(1)  # (B, H, W)\n","            mask_l30_np    = mask_l30.detach().cpu().numpy().squeeze(1)\n","            mask_s1_np     = mask_s1.detach().cpu().numpy().squeeze(1)\n","            mask_planet_np = mask_planet.detach().cpu().numpy().squeeze(1)\n","\n","            B = fake_np.shape[0]\n","            for i in range(B):\n","                pred = fake_np[i]  # (12, H, W)\n","                gt   = gt_np[i]    # (12, H, W)\n","                mask = mask_np[i]  # (H, W)\n","\n","                # Per-pixel MSE / RMSE\n","                sq_err = (pred - gt) ** 2\n","                num_valid = np.sum(mask) * pred.shape[0]\n","                if num_valid > 0:\n","                    mse_val = np.sum(sq_err * mask[np.newaxis, ...]) / num_valid\n","                else:\n","                    mse_val = float(\"nan\")\n","                rmse_val = np.sqrt(mse_val) if not np.isnan(mse_val) else float(\"nan\")\n","\n","                # PSNR / SSIM / SAM\n","                psnr_val = psnr_masked(gt, pred, mask, data_range=1.0)\n","                ssim_val = ssim_eroded_mask(gt, pred, mask, max_val=1.0)\n","                sam_val  = sam_masked(gt, pred, mask)\n","                # Group metrics by input combination\n","                combo = []\n","                if mask_l30_np[i].any():    combo.append(\"L30\")\n","                if mask_s1_np[i].any():     combo.append(\"S1\")\n","                if mask_planet_np[i].any(): combo.append(\"Planet\")\n","                # combo_key = \"+\".join(combo) if combo else \"None\"\n","                if combo:\n","                    combo_key = \"+\".join(combo)\n","                else:\n","                    combo_key = \"None\"\n","                metrics_by_combo[combo_key].append({\"mse\": mse_val, \"rmse\": rmse_val, \"psnr\": psnr_val, \"ssim\": ssim_val, \"sam\": sam_val})\n","\n","                total_mse.append(mse_val)\n","                total_rmse.append(rmse_val)\n","                total_psnr.append(psnr_val)\n","                total_ssim.append(ssim_val)\n","                total_sam.append(sam_val)\n","\n","                # Construct sample name: use original .pt filename (without extension)\n","                full_path = batch[\"__path__\"][i]\n","                sample_name = Path(full_path).stem\n","\n","                pred_save_path = arrays_dir / f\"{sample_name}_pred.tif\"\n","                gt_save_path   = arrays_dir / f\"{sample_name}_gt.tif\"\n","                save_multiband_tif_rasterio(pred, pred_save_path)\n","                save_multiband_tif_rasterio(gt,   gt_save_path)\n","\n","                # Save metric information\n","                results.append({\n","                    \"sample\": sample_name,\n","                    \"mse\": float(mse_val),\n","                    \"rmse\": float(rmse_val),\n","                    \"psnr\": float(psnr_val),\n","                    \"ssim\": float(ssim_val),\n","                    \"sam\": float(sam_val),\n","                })\n","\n","                # Visualize and save: RGB bands use 2,1,0\n","                if pred.shape[0] >= 3:\n","                    pred_rgb = np.clip(pred[[2,1,0]].transpose(1, 2, 0), 0, 1)\n","                    gt_rgb   = np.clip(gt[[2,1,0]].transpose(1, 2, 0), 0, 1)\n","                else:\n","                    pred_rgb = np.repeat(pred[0:1].transpose(1, 2, 0), 3, axis=2)\n","                    gt_rgb   = np.repeat(gt[0:1].transpose(1, 2, 0), 3, axis=2)\n","                diff = np.abs(pred_rgb - gt_rgb)\n","\n","                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","                axes[0].imshow(gt_rgb)\n","                axes[0].set_title(\"Ground Truth (bands 2,1,0)\")\n","                axes[0].axis(\"off\")\n","                axes[1].imshow(pred_rgb)\n","                axes[1].set_title(f\"Prediction (PSNR: {psnr_val:.2f} dB)\")\n","                axes[1].axis(\"off\")\n","                axes[2].imshow(diff, cmap='hot')\n","                axes[2].set_title(\"Absolute Difference\")\n","                axes[2].axis(\"off\")\n","                plt.tight_layout()\n","\n","                # Construct visualization save path\n","                vis_save_path = vis_dir / f\"{sample_name}.png\"\n","                plt.savefig(vis_save_path, dpi=200, bbox_inches='tight')\n","                plt.close(fig)\n","\n","            # Update progress bar\n","            pbar.set_postfix({\n","                \"MSE\": f\"{np.nanmean(total_mse):.6f}\",\n","                \"PSNR\": f\"{np.nanmean(total_psnr):.2f}\",\n","                \"SSIM\": f\"{np.nanmean(total_ssim):.4f}\"\n","            })\n","\n","    # ------------------------------------------------------------------\n","    # 4.4 Aggregate and save metrics\n","    # ------------------------------------------------------------------\n","    summary = {\n","        \"num_samples\": len(total_mse),\n","        \"mse_mean\": float(np.nanmean(total_mse)),\n","        \"mse_std\":  float(np.nanstd(total_mse)),\n","        \"rmse_mean\": float(np.nanmean(total_rmse)),\n","        \"rmse_std\":  float(np.nanstd(total_rmse)),\n","        \"psnr_mean\": float(np.nanmean(total_psnr)),\n","        \"psnr_std\":  float(np.nanstd(total_psnr)),\n","        \"ssim_mean\": float(np.nanmean(total_ssim)),\n","        \"ssim_std\":  float(np.nanstd(total_ssim)),\n","        \"sam_mean\":  float(np.nanmean(total_sam)),\n","        \"sam_std\":   float(np.nanstd(total_sam)),\n","    }\n","\n","    # Save all sample metrics to JSON\n","    results_file = output_dir / \"inference_results.json\"\n","    with open(results_file, \"w\") as f:\n","        json.dump({\n","            \"summary\": summary,\n","            \"details\": results\n","        }, f, indent=2)\n","        # Performance summary by input combination\n","        combo_summaries = {}\n","        for combo_key, records in metrics_by_combo.items():\n","            arr = np.array([[r[\"psnr\"], r[\"ssim\"], r[\"sam\"], r[\"mse\"], r[\"rmse\"]] for r in records])\n","            combo_summaries[combo_key] = {\n","                \"count\": len(records),\n","                \"psnr_mean\": float(np.nanmean(arr[:,0])), \"psnr_std\": float(np.nanstd(arr[:,0])),\n","                \"ssim_mean\": float(np.nanmean(arr[:,1])), \"ssim_std\": float(np.nanstd(arr[:,1])),\n","                \"sam_mean\": float(np.nanmean(arr[:,2])),   \"sam_std\": float(np.nanstd(arr[:,2])),\n","                \"mse_mean\": float(np.nanmean(arr[:,3])),   \"mse_std\": float(np.nanstd(arr[:,3])),\n","                \"rmse_mean\": float(np.nanmean(arr[:,4])),  \"rmse_std\": float(np.nanstd(arr[:,4])),\n","            }\n","        full_summary = {\"summary\": summary, \"by_input_combo\": combo_summaries}\n","        with open(output_dir/\"inference_summary_by_combo.json\", \"w\") as f2:\n","            json.dump(full_summary, f2, indent=2)\n","        print(\"\\n=== Performance by Input Combination ===\")\n","        for combo, stats in combo_summaries.items():\n","            print(f\"{combo}: {stats['count']} samples — PSNR {stats['psnr_mean']:.2f}±{stats['psnr_std']:.2f}, SSIM {stats['ssim_mean']:.3f}±{stats['ssim_std']:.3f}, SAM {stats['sam_mean']:.2f}±{stats['sam_std']:.2f}\")\n","    print(f\"\\n[INFO] Inference completed. Results saved to {results_file}\")\n","    print(f\"       Full 12-band TIFFs saved to {arrays_dir}\")\n","    print(f\"       Visualizations saved to {vis_dir}\")\n","\n","    # Print summary information\n","    print(\"\\nInference Summary:\")\n","    for k, v in summary.items():\n","        if isinstance(v, float):\n","            print(f\"  {k}: {v:.6f}\")\n","        else:\n","            print(f\"  {k}: {v}\")\n","\n","# -------------------------------------------------------------\n","# 5. Command line entry & IDE debugging\n","# -------------------------------------------------------------\n","def parse_args():\n","    parser = argparse.ArgumentParser(\n","        description=\"Inference script for trained GAN generator (save 12-band TIFF)\"\n","    )\n","    parser.add_argument(\"--model\",      type=str, required=True,\n","                        help=\"Model name (MODEL_CLASS must be defined in Models/{model}.py)\")\n","    parser.add_argument(\"--ckpt-path\",  type=str, required=True,\n","                        help=\"Generator checkpoint path (can be entire model or generator_state_dict)\")\n","    parser.add_argument(\"--data-dir\",   type=str, required=True,\n","                        help=\"Data root directory, should contain 'test' subdirectory (consistent with training script)\")\n","    parser.add_argument(\"--output-dir\", type=str, default=\"inference_results\",\n","                        help=\"Inference results output directory (contains TIFF and visualizations)\")\n","    parser.add_argument(\"--batch-size\", type=int, default=16)\n","    parser.add_argument(\"--num-workers\", type=int, default=4)\n","    parser.add_argument(\"--device\",     type=str, default=\"cuda:0\",\n","                        help=\"Device to run inference (e.g., 'cuda:0' or 'cpu')\")\n","    return parser.parse_args()"]},{"cell_type":"markdown","metadata":{"id":"1clNmSaqOXKl"},"source":["# infer-run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EcDfYDkKObTt"},"outputs":[],"source":["print(\"[INFO] Running in IDE debug mode with default settings\")\n","\n","class IDEArgs:\n","    model = \"MultimodalUnetadap_all\"\n","    ckpt_path = \"/content/drive/MyDrive/Colabdata/checkpoints/MultimodalUnetadap_all/best_MultimodalUnetadap_epoch***.pth\"\n","    data_dir = \"/content/data/ReconstructionDataset_Final\"\n","    output_dir = \"/content/drive/MyDrive/Colabdata/inference_results/MultimodalUnetadap_all\"\n","    batch_size = 64\n","    num_workers = 4\n","    device = \"cuda:0\"\n","\n","args = IDEArgs()\n","main(args)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP8LuCtVRNBYlYYZAn2D/ht"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":0}